Admin-focused tool for interacting with ORCM.  This tool has the ability to run as an interactive shell or as a single one-shot command.  Currently the tool provides information about configured resources, sessions, and queues.  The tool is also for managing sessions and power budget at the admin level.


The octl command itself takes the following options:
```
Usage: octl [OPTIONS]
  Open Resilient Cluster Manager "octl" Tool

   -am <arg0>            Aggregate MCA parameter set file list
   -gomca|--gomca <arg0> <arg1>
                         Pass global MCA parameters that are applicable to
                         all contexts (arg0 is the parameter name; arg1 is
                         the parameter value)
-h|--help                This help message
   -omca|--omca <arg0> <arg1>
                         Pass context-specific MCA parameters; they are
                         considered global if --gomca is not used and only
                         one context is specified (arg0 is the parameter
                         name; arg1 is the parameter value)
   -tune <arg0>          Application profile options file list
-v|--verbose             Be Verbose
-V|--version             Show version information

Interactive shell:
Use 'quit' or 'exit' - for exiting the shell
Use '<tab>' or '<?>' for help
```

The subcommands have the option to take arguments specific to that command as well.

#### 3.1.1.1 Interactive CLI
The interactive mode of the CLI is invoked by running the command without any subcommands.  Optional arguments such as MCA parameters can be specified as well.

```
% octl
*** WELCOME TO OCTL ***
 Possible commands:
   resource             Resource Information
   queue                Queue Information
   session              Session Management
   diag                 Diagnostics
   sensor               sensor
   notifier             notifier
   power                Global Power Policy
   grouping             Logical Grouping Information
   Workflow             Workflow information
   query                Query data from DB
   chassis-id           Enable/Disable chassis identify LED.
   quit                 Exit the shell
octl>
```

Once in the interactive shell, the `<tab>` key can be used to either autocomplete unambiguous partial commands or list possible completions for ambiguous partial commands.  The `<?>` key will display more information about all of the commands at the current hierarchy.

For example, pressing `<tab>` after entering `res` will autocomplete the `resource` command, and pressing `<tab>` after the `resource` command is fully entered, will show:
```
octl> resource
        status add remove drain resume
octl> resource
```

As another example, pressing `<?>` after entering `queue`, will show:
```
octl> queue
Possible commands:
   status               Queue Status
   policy               Queue Policies
   define               Queue Definintion
   add                  Add Resource to Queue
   remove               Remove Resource from Queue
   acl                  Queue Access Control
   priority             Queue Priority Modification

octl> queue
```

Notice how in both cases, control is returned to the user to complete the command as desired.

To exit interactive mode type 'quit' or 'exit' from the command prompt.

In the following sections, examples for each command will be given using normal one-shot execution mode.  However, they can also be executed in interactive mode.

#### 3.1.1.2 Resource
The resource command set is used to display information about the resources (nodes) configured in the system.  Currently resource modification is not supported administratively, but once support for that is added, the functionality will be invoked with this command set.  The current implementation displays the node connection state: either up(U), down(D), or unknown(?) and the job state: allocated or unallocated.  The node specification is an ORCM node regex.

Example:
```
% octl resource status
TOTAL NODES : 10
NODES                : STATE  SCHED_STATE
-----------------------------------------
node001              : U      UNALLOCATED
node[3:2-10]         : ?            UNDEF
```

#### 3.1.1.3 Queue
The queue command set displays information about the currently configured queues as well as the sessions within each queue.  The current scheduler defines 3 queues: running, hold, and default.  All sessions get placed initially in the default queue.  As the scheduler launches the sessions they will be placed on the running queue.  And a session that tries to allocate invalid resource definitions will be placed on the hold queue with a string describing the error.

The session information displays:

&lt;session-id&gt; &lt;userid|grouid&gt; &lt;number of nodes requested&gt; &lt;EXclusve or SHared&gt; &lt;Batch or Interactive&gt;

Example:
```
% octl queue status
********
QUEUES
********
running (1 sessions)
----------
1       502|20  1       EX      B
hold (0 sessions)
----------
default (2 sessions)
----------
2       502|20  5       SH      B
3       502|20  2       EX      B
```

#### 3.1.1.4 Session
The session command set is used to modify submitted sessions.  Currently the only modification supported is canceling the session.  If the session is in the default queue waiting to run, canceling the session will remove it from the queue.  If the session is running, then canceling the session will terminate the running jobs and end the session.  The session cancel command takes a session-id as a required argument.

Example:
```
% octl session cancel 1
Success
% octl queue status
********
QUEUES
********
running (0 sessions)
----------
hold (0 sessions)
----------
default (2 sessions)
----------
2       502|20  5       SH      B
3       502|20  2       EX      B
```

#### 3.1.1.5 Diag
The diagnostic command set allows running diagnostics on remote ORCM daemons.  These commands require a node regex specification for determining which remote daemons to run on.  See the [3.3 ORCM Node Regular Expressions](3.3-ORCM-Regex) section for details on how
to construct the regex.

Example 1: run cpu diagnostics on node001 through node010
```sh
% octl diag cpu node[3:1-10]
Success
```

Example 1: run ethernet diagnostics on node001 through node010
```sh
% octl diag eth node[3:1-10]
Success
```

Example 1: run memory diagnostics on node001 through node010
```sh
% octl diag mem node[3:1-10]
Success
```

#### 3.1.1.6 Power
The power budget command set allows setting and retrieving a cluster-wide power budget.  This will only be enforced if the appropriate plugin or subsystems are in place to honor this request.  The power _set_ command takes a required argument of a numeric power budget (in Watts) for the system.  The power _get_ command displays the currently set budget.

Example 1: setting the power budget to 10000 Watts
```sh
% octl power set 10000
Success
```

Example 2: getting the current power budget
```sh
% octl power get
Current cluster power budget: 10000
```

#### 3.1.1.7 Workflow
The analytics command allows the user to add/remove/list workflows from any management node on a cluster.

Example 1: Adding the workflow
```sh
        % octl workflow add workflow.xml [target]
```

After using the above command, user will be able to see the workflow_id on the console. Here Target is the aggregator list and is an optional argument. User can use the target argument and overwrite the aggregator info provided in the XML file

Example 2: list the workflows
```sh
        % octl workflow list target
```

After using the above command, user will be able to see a list of workflows running on a target/aggregator with workflow names and workflow ids

Example 3: Remove the workflow
```sh
        % octl workflow remove target workflow_name workflow_id
```

After using the above command, user will be able to remove a workflow running in the target. Here, wild character "*" is allowed for workflow_name and workflow_id. Remember that if "*" is used for workflow_name,
all the workflows in target will be removed regardless of what the workflow_id will be

#### 3.1.1.8 Sensor Inventory Listing
This command is designed to retrieve from the inventory database the list of sensors for given node(s) that can be used in the Analytics plugins (described immediately above in 3.1.1.7)
The command syntax is:

```
% octl sensor get inventory <node-regex|logical-group> [sensor_search_string]
```

Or in the interactive shell:
```
octl> sensor get inventory <node-regex|logical-group> [sensor_search_string]
```

The node-regex is described in section 3.3 and the logical grouping is described in section 3.5.
The ''sensor_search_string'' at this time is limited to searching the stored plugin names.  Valid formats include omitting the parameter to get <b>all</b> sensors matching the regex or logical name.  Using <nowiki>&apos;&#x2A;&apos;</nowiki> for <i>sensor_search_string</i> does  the same action as omitting <i>sensor_search_string</i>.
Other search syntax include partial plugin names (<u>NOTE:</u> search is done as if your string was followed by the wildcard character '*').

<u>Examples:</u>
```sh
% octl sensor get inventory node01 ip
```
This will match all plugins that start with 'ip' like ipmi.  An equivalent would be:
```sh
% octl sensor get inventory node01 'ip*'
```
This explicitly has the wildcard implied by the first example. All wildcard usage must be a “starts-with” wildcard. In 99% of use cases this is sufficient. If this is not ideal for a specific case then the suggestion would be to make a script filter in bash, python or other language appropriate to the job. An example:
```sh
% octl sensor get inventory node01 '*pm*'
```
This will not match the ipmi plugin and will in-fact return an error.

The output is in standard CSV format with double quotes for both machine parsing (importable into most spreadsheet programs) and human readability. The first line for each new node output is a CSV header with names for the columns. The nodes are output one after another since each node may have different sensors listed in the database. This feature on the OCTL tool does not merge results into one set or union of sensors.

#### 3.1.1.9 Sensor Sample Rate Listing
This command is designed to retrieve the sample rate for the individual sensor for a list of nodes.
The command syntax is:

```
% octl sensor get sample-rate <sensor-name> <node-regex|logical-group>
```

Or in the interactive shell:

```
octl> sensor get sample-rate <sensor-name> <node-regex|logical-group>
```

<u>Examples:</u>

```sh
% octl sensor get sample-rate base c[2:1-10]
```

```sh
% octl sensor get sample-rate coretemp c[2:1-10]
```

```sh
% octl sensor get sample-rate freq c[2:1-10]
```

#### 3.1.1.10 Sensor Policy Listing
This command is designed to retrieve the policies regarding the sensor samples for a list of nodes.
The command syntax is:

```
% octl sensor get policy <node-regex|logical-group>
```

Or in the interactive shell:

```
octl> sensor get policy <node-regex|logical-group>
```

<u>Examples:</u>

```sh
% octl sensor get policy c[2:1-10]
```

#### 3.1.1.11 Sensor Sample Rate Setting
This command is designed to set the sample rate for the individual sensor for a list of nodes. Note
that for per sensor (e.g. coretemp, freq) sample rate, this command will take effect only if the
sensor is started in a progess thread. The setting of the base sample rate does not have this
requirement. The command syntax is:

```
% octl sensor set sample-rate <sensor-name> <node-regex|logical-group>
```

Or in the interactive shell:

```
octl> sensor set sample-rate <sensor-name> <node-regex|logical-group>
```

<u>Examples:</u>

```sh
% octl sensor set sample-rate base 10 c[2:1-10]
```

Setting per sensor sample rate in the examples below will take effect only if the sensor is
started in a progess thread.

<u>Examples:</u>

```sh
% octl sensor set sample-rate coretemp 10 c[2:1-10]
```

```sh
% octl sensor set sample-rate freq 10 c[2:1-10]
```

#### 3.1.1.12 Sensor Sampling Control
The sensor sampling control is designed to enable, disable, or reset the sensor sampling for datagroups (plugin names).  The orcmd service startup state of the sensor data sampling is controlled by the following set of MCA paramaters:
```
sensor_{sensor-name|"base"}collect_metrics = "true" | "false"
```
Using "base" turns on ("true") or off ("false") all sensors loaded using the "sensor" MCA parameter (only loaded sensors can be controlled not plugins excluded from being loaded). Using the sensor\_name instead of "base" overrides the sensor\_base\_collect\_metrics MCA parameter.  The default values of individual datagroup (plugin) is inherited from sensor\_base\_collect\_metrics at orcmd service load time.  This octl command requires the orcmsched be running in the cluster.

The command and interactive shell
```
octl sensor {enable|disable|reset} {node-regex|logical-group} {<datagroup|"all"}
```

For example:
```
$ octl sensor disable node01 all
$ octl sensor enable node01 errcounts
```
After these commands *node01* will only be logging data from the *errcounts* datagroup (plugin).  Then:
```
$ octl sensor reset node01 all
```
restores *node01* to its original sampling state (defined by MCA parameters at orcmd service load time).

For finer control the following syntax can be used
```
octl sensor {enable|disable|reset} {node-regex|logical-group} {datagroup|"all"}:{sensor-label-name}
```

where _sensor-label-name_ is and individual label from the _datagroup_.  This effectively gives control over individual sensor data items.  For example in the _coretemp_ _datagroup_ the sensor labels use the naming convention "core_N_" where _N_ is the zero based core number.  So the following steps will have orcmd sample only "core3" from the "coretemp" _datagroup_ on "node01".
```
$ octl sensor disable node01 all
$ octl sensor enable node01 errcounts:core3
```
The string "all" can also be used for labels except when the _datagroup_ is specified as "all".  Using "all:all" is not legal, instead just use "all".  Also using "all:{sensor-label-name}" is not legal since there is no commonality of sensor-label-name between _datagroups_.

Not all current sensor datagroups respond to sensor sampling control.  Notable exceptions are

* **heartbeat** - This is not a real senor datagroup.
* **dmidata** - No control possible as this doesn't collect periodic data.
* **resusage** - Only datagroup control.  No sensor level control possible.
* **nodepower** - Only datagroup control.  Only one sensor label is returned.
* **mcedata** - Only datagroup control.  Only one sensor label is returned.
* **syslog** - Only datagroup control.  Only one sensor label is returned.


#### 3.1.1.13 Sensor Storage Control
Sensor Storage control is designed to control environmental(raw) and event data. There are couple of MCA parameters provided to control them respectively. analytics\_base\_store\_raw\_data controls environmental data and analytics\_base\_store\_event\_data controls event data.

But, to controls these MCA parameters on fly, following four OCTL commands are provided.

To store Environmental data:

```sh
$ octl sensor store environment_only nodelist
```

Note that this command controls only sensor environment data. It doesn't turn on/off the other storage policies (like event data)

To store Event data:
```sh
$ octl sensor store event_only nodelist
```

Note that this command controls only event data. It doesn't turn on/off the other storage policies (like environment data)

To store both Environmental and Event data
```sh
$ octl sensor store all nodelist
```

To disable storing both Environmental and Event data
```sh
$ octl sensor store none nodelist
```

#### 3.1.1.14 Query

This component is used to query the database in order to obtain basic information of the nodes and its sensors.
In order to get the query component working you need to meet this environment:

* Set the following mca parameters for the scheduler on the orcm-site.xml configuration file:
```xml
<mca-params>
 <value>db_postgres_user=orcmuser:orcmpassword,
		db_postgres_database=orcmdb,
		db_postgres_uri=localhost:5432,
		db_postgres_table=data_sample_raw,
		db_base_verbose=10,
		sensor_base_collect_inventory=true</value>
</mca-params>
```

* Launch an orcmsched service:
```sh
    $ bin/orcmsched
```

* Launch an octl console to call the s:
```sh
    $ bin/octl
```

Whenever you need to specify a node list in a query command you can use:
* Simple comma separated list. Example: host01,host02,host03
* ORCM nodes regular expression: Example: host[2: to specify every node.
* '*' to specify every node.

Query commands:
* HISTORY:
    - SYNTAX:
```sh
        query history [start-date start-time end-date end-time] <nodelist>
```
    - USE: Returns all the data logged by the provided nodes (<nodelist>) during the specified time.
    - EXAMPLE:
```sh
        $ query history 2015-11-13 15:00:00 2015-11-13 16:00:00 master4
```
    - OUTPUT:
```sh
NODE,SENSOR,DATE_TIME,VALUE
master4,procstat_orcmd_pid,2015-10-21 15:31:47.217703,11960,
master4,procstat_running_processes,2015-10-21 15:31:48.219033,0,
master4,procstat_zombie_processes,2015-10-21 15:31:48.219033,0,
3 rows were found (0.091 seconds)
```

* SENSOR:
    - SYNTAX:
```sh
        query sensor <sensor-name> [start-date start-time end-date end-time [upper-bound lower-bound]] <nodelist>
```
    - USE: Returns the logged data corresponding to the given sensor, time and node list. Additionally, the query can be constrained within a range of values defined by an upper and lower bound. Notice that logs containing the bound values will not be included in the result.
    - EXAMPLE:
```sh
        $ query sensor coretemp* 2015-11-13 14:00:00 2015-11-13 16:00:00 0.1 1 master4
```
    - OUTPUT:
```sh
NODE,SENSOR,DATE_TIME,VALUE
master4,coretemp_core 0,2015-10-21 15:31:38.212171,35,degrees C
master4,coretemp_core 1,2015-10-21 15:31:38.212171,35,degrees C
master4,coretemp_core 2,2015-10-21 15:31:38.212171,35,degrees C
master4,coretemp_core 3,2015-10-21 15:31:38.212171,35,degrees C
...
10 rows were found (0.141 seconds)
```
* LOG:
    - SYNTAX:
```sh
        query log [search word] <nodelist>
```
    - USE: Returns the logged data coming from the syslog of the corresponding to the given nodes and search word. The search word accepts the '*' wildcard.
    - EXAMPLE:
```sh
        $ query log *access* master4,c01
```
    - OUTPUT:
```sh
NODE,SENSOR_LOG,MESSAGE
master4,syslog_log_message_0,<86>Oct 28 08:30:29 c01: access granted for user root (uid=0)
c01,syslog_log_message_0,<86>Oct 28 08:30:33 master4: access granted for user root (uid=0)
2 rows were found (0.056 seconds)
```

* IDLE:
    - SYNTAX:
```sh
        query idle [minimum idle time in seconds or HH:MM:SS format] <nodelist>
```
    - USE: Returns the nodes in <nodelist> that has been idle for the given time or more.
    - EXAMPLE:
```sh
        $ query idle 60 master4
```
    - OUTPUT:
```sh
        NODE,IDLE_TIME
        master4,03:13:59.024666
        1 rows were found (0.016 seconds)
```

* STATUS:
    - SYNTAX:
```sh
        query node status <nodelist>
```
    - USE: Returns the status logged in the data base for the nodes in <nodelist>.
    - EXAMPLE:
```sh
        $ query node status master4
```
    - OUTPUT:
        There is no output for now as this query returns the field "status" on the "node" table and this field is not being populated yet.

* EVENT
    - DATA:
        - SYNTAX:
```sh
            query event data [start-date start-time end-date end-time] <nodelist>
```
        - USE: Returns events from database.
        - EXAMPLE:
```sh
           $query event data 2016-02-16 08:22:00 2016-02-16 08:22:14 master4
```
         - OUTPUT:
```sh
EVENT_ID,DATE_TIME,SEVERITY,TY:PE,HOSTNAME,EVENT_MESSAGE
66846,2016-02-16 08:22:04,CRITICAL,EXCEPTION,master4,core 0 value 44.00 degrees C,greater than threshold 25.00 degrees C
66847,2016-02-16 08:22:04,CRITICAL,EXCEPTION,master4,core 1 value 42.00 degrees C,greater than threshold 25.00 degrees C
66865,2016-02-16 08:22:09,CRITICAL,EXCEPTION,master4,core 2 value 44.00 degrees C,greater than threshold 25.00 degrees C
66866,2016-02-16 08:22:09,CRITICAL,EXCEPTION,master4,core 3 value 41.00 degrees C,greater than threshold 25.00 degrees C
66881,2016-02-16 08:22:09,CRITICAL,EXCEPTION,master4,core 4 value 45.00 degrees C,greater than threshold 25.00 degrees C
66882,2016-02-16 08:22:09,CRITICAL,EXCEPTION,master4,core 5 value 46.00 degrees C,greater than threshold 25.00 degrees C
6 rows were found (0.396 seconds)
```

    - SENSOR-DATA:
        - SYNTAX:
```sh
            query event sensor-data <event-id> before/after <minutes> <sensor> <nodelist>
```
        - USE: Returns the sensor data around an event.
        - EXAMPLE:
```sh
            $ query event sensor-data 1 after 10 coretemp* master4
```
        - OUTPUT:

```sh
DATE_TIME,HOSTNAME,DATA_ITEM,VALUE,UNITS
2016-02-09 13:53:52,master4,coretemp_core 0,23,degrees C
2016-02-09 13:53:52,master4,coretemp_core 1,23,degrees C
2016-02-09 13:53:52,master4,coretemp_core 2,23,degrees C
3 rows were found (0.396 seconds)
```

Please notice that due to the amount of information that could result from the usage of the history or sensor subcommands, this tool is currently limiting those subcommands to return only 100 rows. If users need to inspect more data or require more SQL-oriented functionality, they are advised to access the DB directly by other means.

#### 3.1.1.15 Notifier

The 'notifier' commands are used for configuring  and querying the policies for
error and exception notification during runtime.

* Following command line options are used for setting up severity levels and corresponding actions:

* NOTIFIER:
    - SYNTAX:
```sh
        notifier set policy <severity> <action> <nodelist>
```
    - USE: Returns success - when operation is successful or Returns failed - with an error message.
    - EXAMPLE:
```
$ octl notifier set policy emerg  smtp   rack01
$ octl notifier set policy alert  smtp   rack01
$ octl notifier set policy crit   smtp   rack01
$ octl notifier set policy error  syslog rack01
$ octl notifier set policy warn   syslog rack01
$ octl notifier set policy notice syslog rack01
$ octl notifier set policy debug  syslog rack01
```
    - OUTPUT:

```sh
        Notifier set policy on Node:rack01

        Success
```

* Following command line options are used for retrieving severity levels and corresponding actions:

* NOTIFIER:
    - SYNTAX:
```sh
        notifier get policy <nodelist>
```
    - USE: Returns success - when operation is successful or Returns failed - with an error message.
    - EXAMPLE:
```sh
        $ octl notifier get policy rack01
```
    - OUTPUT:
```
Node          Severity      Action
-----------------------------------
rack01        EMERG         smtp
rack01        ALERT         syslog
rack01        CRIT          syslog
rack01        ERROR         syslog
rack01        WARN          syslog
rack01        NOTICE        syslog
rack01        INFO          syslog
rack01        DEBUG         syslog
```
* Following command line options are used for changing the existing smtp configuration:

* NOTIFIER:
    - SYNTAX:
```sh
        notifier set smtp-policy <key> <value> <nodelist>
```
    - USE: Returns success - when operation is successful or Returns failed - with an error message.
    - EXAMPLE:
```sh
$ octl notifier set smtp-policy server_name <email-server>  rack01
$ octl notifier set smtp-policy server_port <portno>  rack01
$ octl notifier set smtp-policy to_addr <email-addr>  rack01
$ octl notifier set smtp-policy from_addr <email-addr>  rack01
$ octl notifier set smtp-policy from_name <name>  rack01
$ octl notifier set smtp-policy subject <email-subject>  rack01
$ octl notifier set smtp-policy body_preffix <email-body-preffix>  rack01
$ octl notifier set smtp-policy body_suffix <email-body-suffix>  rack01
$ octl notifier set smtp-policy priority <1-high,2-normal,3-low>  rack01
```
    - OUTPUT:

```
        Notifier set smtp-policy on Node:rack01

        Success
```

* Following command line options are used for retrieving smtp configuration information from a given nodelist:

* NOTIFIER:
    - SYNTAX:
```sh
        notifier get smtp-policy <nodelist>
```
    - USE: Returns success - when operation is successful or Returns failed - with an error message.
    - EXAMPLE:
```sh
        $ octl notifier get smtp-policy rack01
```
    - OUTPUT:
```
NODE            SMTP_KEY          SMTP_VALUE
-----------------------------------------------
rack01          server_name   emailserver.com
rack01          server_port   25
rack01          to_addr       admin@email.com
rack01          from_addr     system@email.com
rack01          from_name     RAS Monitoring System
rack01          subject       EVENT-NOTIFICATION
rack01          body_prefix   NOTIFICATION MESSAGE BEGIN
rack01          body_suffix   NOTIFICATION MESSAGE END
rack01          priority      1
```


#### 3.1.1.16 Chassis-id
The __chassis-id__ command is used for enabling or disabling the chassis ID LED from a given node. Every __chassis-id__ action will be stored as an event into database.  The command sintax is:
```sh
octl > chassis-id {state|enable [seconds]|disable} <node_list>
```

Where _node_list_ could be
* ORCM regex
* logical group
* a node list separated by a comma


The __state__ sub-command retrieves the current state of the chassis ID LED on the requested nodes.
```
Usage:
    octl> chassis-id state myNode

Output:

    Node          Chassis ID LED
    -----------------------------------
    myNode               OFF
```

The __enable__ sub-command turns ON the chassis ID LED on the requested nodes following the next rules:
* The chassis ID LED will be turned ON the specified __seconds__.
* If no __seconds__ are specified, the chassis ID LED will be turned ON indefinitely.
* The maximun value for __seconds__ is 255 seconds.
```
Usage:
    octl> chassis-id enable [seconds] myNode

Output:
    None
```

The __disable__ sub-command turns OFF the chassis ID LED on the requested nodes
```
Usage:
    octl> chassis-id disable myNode

Output:
    None
```
__NOTE__: This command is hardware-dependent and it might not be supported on all platforms.
