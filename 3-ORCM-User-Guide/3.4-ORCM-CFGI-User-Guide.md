# Introduction
## Purpose of this document
This document hopes to instruct the reader on how to create and maintain an ORCM CFGI file.
This file is used to tell the ORCM software the configuration of the cluster.
This document is intended for ORCM developers and system administrators in charge 
of installing ORCM and/or maintaining an ORCM installation.

This document assumes the reader is familiar with ORCM and its architecture 
and with the system they are trying to configure.  This document is not a user guide or manual for ORCM.

## Reference documents
The reader may find these reference documents helpful in understanding this user guide.
* Open MPI code internals        http://www.open-mpi.org/video/?category=internals
* ORCM Developer Information     https://svn.open-mpi.org/trac/orcm
* Backus-Naur Notation           http://en.wikipedia.org/wiki/Extended_Backus%E2%80%93Naur_Form
* ORCM Regex                     https://github.com/intel-ctrlsys/sensys/wiki/3.3-ORCM-Regex



# ORCM CFGI file syntax
The CFGI file in a plain text ASCII file written in XML format.  The grammar in the file is defined by the Backus-Naur desciption shown hereafter.  Details for each entry will be given afterwards.

* **configuration** ::= **version** **creation** **role** **junction** [**scheduler**]
* **naming** ::= **name**
* **junction** ::= **type** **naming** [**controller**] **junction**
* **controller** ::= **host** [**port**] [**cores**] [**log**] [**envar**] [**aggregator**]**mca-params***
* **scheduler** ::= **shost** **port** [**mca-params**] [**aggregator**]

In order to simplify the grammar specification, the basic data types are as follows:

* **string** ::= a sequence of ASCII characters with value in [33, 126]
* **regex** ::= a **string** where the characters, [ ] :, have special meaning.  
       See the above link on ORCM Regex for details
* **uint** ::= a **string** of an unsigned 4-bytes integer as defined in the C language
* **ushort** ::= a **string** of an unsigned 2-bytes integer as defined in the C language
* **float** ::= a **string** of a floating point number
* **csList** ::= a **string** containing different fields separated by commas
A **string** does not need to be delimited with double quotes if it does not contain whitespace characters.  Otherwise, it needs to be delimited with double quotes, if the white spaces are to be preserved.
-->At this time, single quotes cannot be used to delimit **string**.

Because the CFGI is written in XML, the above tokens will have an appropriate representation. For example, the command **configuration**, in XML, is represented by the pair of tags < configuration > … < /configuration > where the ellipsis here will hold the rest of the configuration.

## Details of the CFGI grammar
Each command line with be repeated here, and followed by details.  Items with a full description, like <junction>, will be addressed when their full description are presented.

**configuration** ::= **version** **creation** **role** **junction** [**scheduler**]

* **version** ::= A **float** indicating the version and subversion numbers. The format is as follows "X.Y" where X and Y are any integer; Y is the sub-version.
* **creation** ::= A <string> to identify time, date and who build this configuration files. At this time, the format is left to the user’s discretion.
* **role** ::= RECORD  .  Only one key word at this time: RECORD.  It is not case sensitive.  The key word RECORD refers to the creation of a configuration record.  There can be only one configuration with the modifier RECORD.

**naming** ::= **name**
* **name** ::= A **regex** expanding to one or more names.  A special character, ‘@’, is reserved to indicate that the name of the parent junction in the hierarchy will be used to replace the ‘@’ character.  For example, if the parent junction is named “rack1” and its child junction as for <name>=”@_node”, then the child junction’s name string is “rack1_node”, where “rack1” of the parent replaced the ‘@’ character. 
       
**junction** ::= **type** **naming** [**controller**] **junction**
* **type** ::= ("cluster"|"row"|"rack"|"node")  .  "cluster" is always the root of the hierarchy.  There can only be one junction of type cluster.  It must always be mentioned.
_"row" & "rack" are used in a 4-tiers hierarchy: cluster, row, rack, node.  "cluster" is always the root of the hierarchy; and there can be only one cluster.  "node" are always leaf points in the hierarchy.  With “cluster” as root and “node” as leaf, the hierarchy always has a unique start point and well defined ending points.  Each junction must have a name.  Furthermore the name of siblings must be different.  For example, if a cluster contains two rows, these rows must not have the same name.  Currently, at most 4-tiers of hierarchy are supported and exactly the following order: cluster, row, rack, node.  If a row or a rack is omitted, a fictitious equivalent will be automatically inserted.

**controller** ::= **host** **port** [**cores**] [**log**] [**envar**] **mca-params***

* **host** ::= A **regex** of an actual IP address or an IP resolvable name, of the machine hosting the controller.  It can include the hierarchical operator ‘@’ as explained in <name>.  If it does, this ‘@’ operator will refer to the immediate junction hosting this controller. There is a strong relationship between the hosting junction name and its controller host value.
If the hosting junction’s name is a **regex**, one must use ‘@’ as the controller host value.
If the hosting junction’s name is a **cstring** and not a **regex**, then the controller host value can also be a **cstring**.
EXCEPTION: For node junction, if the hosting junction’s name is a **cstring** and not a **regex**, then its controller host value must have be exactly that node junction’s name.
NOTE: When in doubt about a controller host name, use ‘@’.

* **port** ::= an **ushort** for the ID of the listened port.

* **cores** ::= a **regex** specifying the ID of the cores to be used

* **log** ::= **string** storing the output log file path and name.

* **envar** ::=  **cslist** of **string** of environment variable and their values.

* **mca-params** ::= **cslist** of **string**, where each **string** contains a single tag-value pairs, with the “=” as separator.  Use multiple statements for multiple specification.  All multiple statements must be grouped together in the XML file.

* **aggregator** ::= (yes|no)  .  If yes, than that particular junction or scheduler will designated as an accumulator.

**scheduler** ::= **shost** **port** [**mca-params**] [**aggregator**] **queues***

* **shost** ::= The same as **host** but specific to the **scheduler**

# XML example

Typically the file is written in the directory $PATH2ORCM/orcm/etc/orcm-site.xml.  ORCM will look for this file by name.

The ORCM XML parser only parse a simplified XML format.  The simplification are as follows:
* XML attributes are not supported
* Quoted strings can only use double quotes “.

A prototype CFGI file written in XML is provided hereafter.  It presents a cluster with the following configuration:

* A 4-tier hierarchy: cluster, row, rack, junction
* 1 Scheduler
* 1 row named row1 without a controller
* 4 racks in the single row, locally called "agg01", "agg02", "agg03", "agg04"
* 1024 nodes equally distributed among the racks, locally called “node0000”, …, “node1023”
* Each rack and node has a controller

The example has in-lined comments which provides further details.
```
<?xml version="1.0" encoding="UTF-8" ?>
<configuration>
    <!-- Version is fixed to 3.0 -->
    <version>3.0</version>
    <!-- We need a single RECORD -->
    <role>RECORD</role>
    <junction>
        <!-- We need a single root for the hierarchy -->
        <type>cluster</type>
        <name>master3</name>
        <junction>
            <type>row</type>
            <name>row1</name>
            <junction>
                <type>rack</type>
                <name>agg01</name>
                <controller>
                    <host>agg01</host>
                    <port>55805</port>
                    <aggregator>yes</aggregator>
                </controller>
                <junction>
                    <type>node</type>
                    <name>node[4:0-255]</name>
                    <controller>
                        <!-- This controller takes its host name from its row’s name -->
                        <!-- The @ operator does the unique selection -->
                        <host>@</host>
                        <port>55805</port>
                        <aggregator>no</aggregator>
                    </controller>
                </junction>
            </junction>
            <junction>
                <type>rack</type>
                <name>agg02</name>
                <controller>
                    <host>agg02</host>
                    <port>55805</port>
                    <aggregator>yes</aggregator>
                </controller>
                <junction>
                    <type>node</type>
                    <name>node[4:256-511]</name>
                    <controller>
                        <host>@</host>
                        <port>55805</port>
                        <aggregator>no</aggregator>
                    </controller>
                </junction>
            </junction>
            <junction>
                <type>rack</type>
                <name>agg03</name>
                <controller>
                    <host>agg03</host>
                    <port>55805</port>
                    <aggregator>yes</aggregator>
                </controller>
                <junction>
                    <type>node</type>
                    <name>node[4:512-767]</name>
                    <controller>
                        <host>@</host>
                        <port>55805</port>
                        <aggregator>no</aggregator>
                    </controller>
                </junction>
            </junction>
            <junction>
                <type>rack</type>
                <name>agg04</name>
                <controller>
                    <host>agg04</host>
                    <port>55805</port>
                    <aggregator>yes</aggregator>
                </controller>
                <junction>
                    <type>node</type>
                    <name>node[4:768-1023]</name>
                    <controller>
                        <host>@</host>
                        <port>55805</port>
                        <aggregator>no</aggregator>
                    </controller>
                </junction>
            </junction>
        </junction>
    </junction>
    <scheduler>
        <!—shost identifies the node that houses the ORCM scheduler. Only one allowed -->
        <shost>master01</shost>
        <port>55820</port>
    </scheduler>
</configuration>
```

# Running multiple aggregators on one node

Currently, ORCM supports running multiple aggregators on one node. The reason to support this is
that at extreme scales, a single aggregator will be saturated given the large number of compute nodes
and extreme volume of data. In this case, multiple aggregators will be needed. However, we do not
want each aggregator to run on separate node, because we want as many compute nodes as possible to
run applications. Given that a compute node will likely have much higher parallelism (e.g 1000 cores)
at extreme scales, running multiple aggregators on one node will be a good choice.

To run multiple aggregators on the same node, ORCM needs to distinguish between them with the combination
of logical hostname and port number. Assuming the actual hostname of the node is master01, and the external ip address of the node is: X.X.X.X. In the /etc/hosts file, there should be one line specifying the mapping of the actual hostname and the external ip address with the format:

```
X.X.X.X   master01
```

For example, if the user/admin wants to run 4 aggregators on the same node, he/she can define the logical hostnames (aliases) for the node by appending the aliases to master01 in the /etc/hosts file as follows assuming
that the aliases are agg01, agg02, agg03 and agg04:

```
X.X.X.X   master01 agg01 agg02 agg03 agg04
```

The mapping of the logical hostnames to the node ip for the aggregators needs to be copied to the
corresponding compute nodes as well in their /etc/hosts files, in order for the compute nodes to
recognize the logical hostnames of the aggregators.

In the configuration file, each aggregator must be given an unique logical hostname, as well as an
unique port number. When running multiple aggregators on the same node, each aggregator needs to specify
the unique port number (the exact same ones in the configuration file) with the mca parameter
"--omca cfgi\_base\_port_number", or with the "-p" option.


An example to configure 4 aggregators on the same node with hostname master01 is shown as follows:

```
<?xml version="1.0" encoding="UTF-8" ?>
<configuration>
    <version>3.0</version>
    <role>RECORD</role>
    <junction>
        <type>cluster</type>
        <name>default_cluster</name>
        <junction>
            <type>row</type>
            <name>default_row</name>
            <junction>
                <type>rack</type>
                <name>rack1</name>
                <controller>
                    <host>agg01</host>
                    <port>55805</port>
                    <aggregator>yes</aggregator>
                </controller>
                <junction>
                    <type>node</type>
                    <name>node[4:0-255]</name>
                    <controller>
                        <host>@</host>
                        <port>55805</port>
                        <aggregator>no</aggregator>
                    </controller>
                </junction>
            </junction>
            <junction>
                <type>rack</type>
                <name>rack2</name>
                <controller>
                    <host>agg02</host>
                    <port>55806</port>
                    <aggregator>yes</aggregator>
                </controller>
                <junction>
                    <type>node</type>
                    <name>node[4:256-511]</name>
                    <controller>
                        <host>@</host>
                        <port>55805</port>
                        <aggregator>no</aggregator>
                    </controller>
                </junction>
            </junction>
            <junction>
                <type>rack</type>
                <name>rack3</name>
                <controller>
                    <host>agg03</host>
                    <port>55807</port>
                    <aggregator>yes</aggregator>
                </controller>
                <junction>
                    <type>node</type>
                    <name>node[4:512-767]</name>
                    <controller>
                        <host>@</host>
                        <port>55805</port>
                        <aggregator>no</aggregator>
                    </controller>
                </junction>
            </junction>
            <junction>
                <type>rack</type>
                <name>rack4</name>
                <controller>
                    <host>agg04</host>
                    <port>55808</port>
                    <aggregator>yes</aggregator>
                </controller>
                <junction>
                    <type>node</type>
                    <name>node[4:768-1023]</name>
                    <controller>
                        <host>@</host>
                        <port>55805</port>
                        <aggregator>no</aggregator>
                    </controller>
                </junction>
            </junction>
        </junction>
    </junction>
    <scheduler>
        <shost>master01</shost>
        <port>55820</port>
    </scheduler>
</configuration>
```

To run the 4 aggregators on the same node, each aggregator needs to specify its port number as follows:

% ./orcmd --omca cfgi\_base\_port_number 55805
% ./orcmd --omca cfgi\_base\_port_number 55806
% ./orcmd --omca cfgi\_base\_port_number 55807
% ./orcmd --omca cfgi\_base\_port_number 55808

or use the short -p option as follows:

% ./orcmd -p 55805
% ./orcmd -p 55806
% ./orcmd -p 55807
% ./orcmd -p 55808
