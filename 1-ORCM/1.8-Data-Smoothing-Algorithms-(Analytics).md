Analytics is a framework to run user defined data smoothing algorithms on any source data that comes to the framework. Analytics provide different plug-ins which can do the analysis on the source data based on users request. A user can request the analysis of source data through workflows. Below is the sample of a workflow file:

	nodelist:localhost
	component:filter:args:hostname=<node_name>;data_group=<sensor_name>;core=<core_id>
	component:average:args:db=yes

In which, the <node_name>, <sensor_name> and <core_id> could be a comman separated list. For example, if user specifies "hostname=node1;data_group=coretemp;core=core 1,core 2" in the filter, and the average plugin will do an average of the coretemp values of core 1 and core 2.

The below diagram provides an overview of Analytics Framework in a cluster environment and different analytics plug-ins are descibed later in this page.
Analytics framework runs only on the aggregator node. The actions to be performed by the Analytics framework are submitted by OCTL tool. The OCTL tool can be run by administrator from any management nodes from network. The details of implementation of OCTL tool is provided in Step 1 of this document. The details of implementation of Analytic framework are provided in Step 2 of this document.

![Analytics Framework Overview] (1-ORCM/Overview.png)

Step 1: OCTL

![OCTL Overview] (1-ORCM/OCTL_Overview.png)

![OCTL Flow Chart] (1-ORCM/OCTL_Flow.png)

Note: Please refer to the OCTL tool wiki page for the usage.

Step 2: Analytics Framework

![Working of Analytics Framework] (1-ORCM/Analytics_details.png)

![Analytics Framework Flow Chart] (1-ORCM/Analytics_flow.png)

There are currently three plug-ins (Filter, Average, and Threshold) are supported in Analytics Framework and in addition to these plug-ins there is a DB Storage Attribute is given which will allow the user to log the data into database. Short description of all the plug-ins are given below.

Filter:

The role of the FILTER plug-in is to compare the user requested information with the actual source data sent by the aggregator. By doing this we make sure that we analyze only user requested data instead of analyzing any unnecessary data. After validating the data this plug-in will pass on this information to average or any other plug-in. The first workflow step in a workflow is always the Filter, which is mandatory.

Average:

Currently, the average is the running average of all the data up to the current data value. Average plug-in utilizes the data received from the previous workflow step. The formula is as follows:
	A(N+1) = (N * A(N) + new_sample) / (N + 1)

Threshold:

The threshold plugin does level checking on data and generates syslog/email notification for all out-of-bounds values. User shall specify high and low bounds using workflow parameters. Example:

component:threshold:args:policy=hi|100|crit|email;lo|20|warn|syslog, which means that for values higher than 100, send an email reporting a critical event, and for values lower than 20, write a warning message to the syslog

Sliding window:

The sliding window plugin calculates the statistics (average, min, max, and standard deviation (sd)) of the values of the coming samples within a time/counter window. The boundaries (left and right) of the window are incrementing (sliding) with a sliding size. In the current implementation, the sliding size equals to the window size. Below are two sample workflow steps for the sliding window plugin:

	component:window:args:window_size=1;unit=hour;compute=average;type=time
	component:window:args:window_size=10;compute=sd;type=counter 

Definitions of the attributes of sliding window:
* window_size: size of the window (mandatory)
* compute: the type of computation (average, min, max, s.d.) to be done (mandatory). For now, only one computation is allowed in one workflow step.
* type: the type of the window (optional, default=time)
	o type=time: the window is a time window. By default, the unit of the window_size is second. When the users want to do computation with units of minute, hour, day. We are not supporting units longer than day(e.g. weeks, months, years, etc). The users can either set the unit with a unit attribute, or convert the value to second. For example: window_size=3600 means a one-hour window, this equals to set: window_size=1;unit=hour
	o type=counter: the window is a counter window, meaning the number of samples. For example: window_size=200 means doing computation for the last 200 samples. 

DB Storage Attribute:

The role of the DB storage attribute is to store the analyzed data in the database. This attribute is optional and will be activated only if user has requested the data to be stored in the database.
Example

	component:average:args:db=yes

Using an MCA parameter from the analytics code, sensor data logging to Database is set to false. But, Sensor data logging is turned to true in the build config files. The MCA parameter that controls the sensor data logging to DB is "analytics_base_set_db_logging"
    orcmd --omca analytics_base_set_db_logging true

