Analytics is a framework to run user defined data smoothing algorithms on any source data that comes to the framework. Analytics provide different plug-ins which can do the analysis on the source data based on users request. A user can request the analysis of source data through workflows. Below is the sample of a workflow file:

	nodelist:localhost
	component:filter:args:hostname=<node_name>;data_group=<sensor_name>;core=<core_id>
	component:average:args:db=yes

In which, the <node_name>, <sensor_name> and <core_id> could be a comman separated list. For example, if user specifies "hostname=node1;data_group=coretemp;core=core 1,core 2" in the filter, and the average plugin will do an average of the coretemp values of core 1 and core 2.

The below diagram provides an overview of Analytics Framework in a cluster environment and different analytics plug-ins are descibed later in this page.
Analytics framework runs only on the aggregator node. The actions to be performed by the Analytics framework are submitted by OCTL tool. The OCTL tool can be run by administrator from any management nodes from network. The details of implementation of OCTL tool is provided in Step 1 of this document. The details of implementation of Analytic framework are provided in Step 2 of this document.

![Analytics Framework Overview] (1-ORCM/Overview.png)

Step 1: OCTL

![OCTL Overview] (1-ORCM/OCTL_Overview.png)

![OCTL Flow Chart] (1-ORCM/OCTL_Flow.png)

Note: Please refer to the OCTL tool wiki page for the usage.

Step 2: Analytics Framework

![Working of Analytics Framework] (1-ORCM/Analytics_details.png)

![Analytics Framework Flow Chart] (1-ORCM/Analytics_flow.png)

There are currently four plug-ins (Filter, Aggregate, Threshold and Sliding window) are supported in Analytics Framework and in addition to these plug-ins there is a DB Storage Attribute is given which will allow the user to log the data into database. Short description of all the plug-ins are given below.

Filter:

The role of the FILTER plug-in is to compare the user requested information with the actual source data sent by the aggregator. By doing this we make sure that we analyze only user requested data instead of analyzing any unnecessary data. After validating the data this plug-in will pass on this information to average or any other plug-in. The first workflow step in a workflow is always the Filter, which is mandatory.

Aggregate:

The aggregate plugin performs running average, minimum and maximum aggregation operations.The running average is the average of all the data up to the current data value. Average plug-in utilizes the data received from the previous workflow step. The formula is as follows:
	A(N+1) = (N * A(N) + new_sample) / (N + 1)
Example:
component:aggregate:args:operation=<average/min/max>

Threshold:

The threshold plugin does level checking on data and generates syslog/email notification for all out-of-bounds values. User shall specify high and low bounds using workflow parameters. Example:

component:threshold:args:policy=hi|100|crit|email;lo|20|warn|syslog, which means that for values higher than 100, send an email reporting a critical event, and for values lower than 20, write a warning message to the syslog

Sliding window:

The sliding window plugin calculates the statistics (average, min, max, and standard deviation (sd)) of the values of the coming samples within a time/counter window. The boundaries (left and right) of the window are incrementing (sliding) with a sliding size. In the current implementation, the sliding size equals to the window size. Below are two sample workflow steps for the sliding window plugin:

	component:window:args:win_size=1;unit=hour;compute=average;type=time
	component:window:args:win_size=10;compute=sd;type=counter 

Definitions of the attributes of sliding window:
* wind_size: size of the window (mandatory)
* compute: the type of computation (average, min, max, s.d.) to be done (mandatory). For now, only one computation is allowed in one workflow step.
* type: the type of the window (optional, default=time)
	o type=time: the window is a time window. By default, the unit of the win_size is second. When the users want to do computation with units of minute, hour, day. We are not supporting units longer than day(e.g. weeks, months, years, etc). The users can either set the unit with a unit attribute, or convert the value to second. For example: win_size=3600 means a one-hour window, this equals to set: win_size=1;unit=hour
	o type=counter: the window is a counter window, meaning the number of samples. For example: win_size=200 means doing computation for the last 200 samples. 

DB Storage Attribute:

The role of the DB storage attribute is to store the analyzed data in the database. This attribute is optional and will be activated only if user has requested the data to be stored in the database.
Example

	component:average:args:db=yes

Using an MCA parameter from the analytics code, sensor data logging to Database is set to false. But, Sensor data logging is turned to true in the build config files. The MCA parameter that controls the sensor data logging to DB is "analytics_base_set_db_logging"
    orcmd --omca analytics_base_set_db_logging true

##Count Over Time Threshold (cott)

This analytics plugin is designed to take the incoming absolute count data during sampling and will fire an event when the calculated count delta is greater than or equal to the threshold value within the specified time window.  The 'errcounts' sensor plugin can be used as direct input to this analytic plugin.

Understand that the input values for a given data label (or key) is the current count and not a change of the count since the last data associated with the label.  This means if you get a change value you will need an accumulator-like plugin to get a count to input into this plugin.

The output of this plugin is the incoming data.  It only examines and triggers events.

__Sample Workflow File:__
<pre>
nodelist:node[3:1-254]
component:filter:args:data_group=errcounts
component:cott:args:severity=error;fault_type=soft;store_event=yes;notifier_action=email;label_mask=CPU_SrcID#*_Channel#*_DIMM#*_CE;time_window=60s;count_threshold=10
</pre>

In this example, the filter returns only data from the errcounts plugin to use as input to the cott analytics plugin.  The plugin looks for a count change of 10 within a time window of 60 seconds.  The example when triggered will fire an event of "error" severity and as a "soft" fault.  It also will store the event in the event database.

__Workflow Arguments:__

* ___label\_mask___ Required; This specifies the data labels (key) to match using wildcard syntax.
* ___count\_threshold___  Required; This denotes the number of new item counts within the time_window which after the the event is fired (inclusive).
* ___severity___ Optional; This defaults to "error" and can only be "emerg", "alert", "crit", "error", "warn".
* ___fault\_type___ Optional; This defaults to "hard" and can only be "hard" or "soft".
* ___store\_event___ Optional; This defaults to "yes" and can only be "yes" or "no".
* ___notifier\_action___  Optional; This defaults to "none" and can only be "none", "email", "syslog".
* ___time\_window___ Optional; This defaults to 1 second; the format is a number followed by a character denoting the unit of the number:
 * __s__ - seconds
 * __m__ - minutes
 * __h__ - hours
 * __d__ - days

