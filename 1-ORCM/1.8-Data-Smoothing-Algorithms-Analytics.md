Analytics is a framework to run user defined data smoothing algorithms on any source data that comes to the framework. Analytics provide different plug-ins which can do the analysis on the source data based on users request. A user can request the analysis of source data through workflows. Below is the sample of a workflow file:

    nodelist:localhost
    component:filter:args:hostname=<node_name>;data_group=<sensor_name>;core=<core_id>
    component:average:args:db=yes

In which, the <node_name>, <sensor_name> and <core_id> could be a comma separated list. For example, if user specifies "hostname=node1;data_group=coretemp;core=core 1,core 2" in the filter, and the average plugin will do an average of the coretemp values of core 1 and core 2.

The below diagram provides an overview of Analytics Framework in a cluster environment and different analytics plug-ins are descibed later in this page.
Analytics framework runs only on the aggregator node. The actions to be performed by the Analytics framework are submitted by OCTL tool. The OCTL tool can be run by administrator from any management nodes from network. The details of implementation of OCTL tool is provided in Step 1 of this document. The details of implementation of Analytic framework are provided in Step 2 of this document.

![Analytics Framework Overview] (1-ORCM/Overview.png)

Step 1: OCTL

![OCTL Overview] (1-ORCM/OCTL_Overview.png)

![OCTL Flow Chart] (1-ORCM/OCTL_Flow.png)

Note: Please refer to the OCTL tool wiki page for the usage.

Step 2: Analytics Framework

![Working of Analytics Framework] (1-ORCM/Analytics_details.png)

![Analytics Framework Flow Chart] (1-ORCM/Analytics_flow.png)

There are currently six plug-ins (Filter, Aggregate, Threshold, Window, Cott and Spatial) are supported in Analytics Framework. In addition to these plug-ins, there is a DB Storage Attribute which will allow the plug-in to log the data into database when given. Besides, there are a couple of MCA parameters that would allow user to determine whether log the raw data and event data to database or not. Short description of all the plug-ins, attribute, and the mca parameters are given below.


##1. Filter

The role of the FILTER plug-in is to compare the user requested information with the actual source data sent by the aggregator. By doing this we make sure that we analyze only user requested data instead of analyzing any unnecessary data. After validating the data this plug-in will pass on this information to average or any other plug-in. The first workflow step in a workflow is always the Filter, which is mandatory. 

__1.1 Workflow Example:__

    nodelist:aggregator1
    component:filter:args:hostname=c[2:1-8];data_group=coretemp;core id=core 0

In the above example, the first line of nodelist indicates which aggregator(s) this workflow will be submitted to. The filter workflow step has three attributes, namely hostname, data_group, and core id. These attributes match the "key" of the source data (in this case, the sensor). This example filters the core 0's coretemp data for all the nodes c01, c02, ..., c08. 


##2. Aggregate

The aggregate plugin performs running average, minimum and maximum aggregation operations.The running average is the average of all the data up to the current data value. Average plug-in utilizes the data received from the previous workflow step. The formula for doing average is as follows:
    
    A(N+1) = (N * A(N) + new_sample) / (N + 1)

__2.1 Workflow Example:__

    nodelist:aggregator1
    component:filter:args:hostname=c01,c02;data_group=coretemp
    component:aggregate:args:operation=average

In the above example, the coretemp sensor data of nodes c01 and c02 will be passed down to the aggregate plugin to do an average across c01 and c02. 

__2.2 Aggregate plugin attributes:__

* __operation:__ (_Required_) This specifies the computation type. Currently, there are three types of computations supported, namely average, min and max.


##3. Threshold

The threshold plugin does level checking on data and generates syslog/email notification for all out-of-bounds values. User shall specify high and low bounds using workflow parameters. 

__3.1 Workflow Example:__

    nodelist:aggregator1
    component:filter:args:hostname=c01,c02;data_group=coretemp
    component:threshold:args:policy=hi|100|crit|email;lo|20|warn|syslog

The above workflow means that for values higher than 100, send an email reporting a critical event, and for values lower than 20, write a warning message to the syslog


##4. Window

The window plugin calculates the statistics (average, min, max, and standard deviation (sd)) of the values of the coming samples within a time/counter window. The boundaries (left and right) of the window are incrementing (sliding) with a sliding size. In the current implementation, the sliding size equals to the window size. 

__4.1 Workflow Example:__

_Workflow 1:_

    nodelist:aggregator1
    component:filter:args:hostname=c02;data_group=coretemp
    component:window:args:win_size=1;unit=hour;compute=average;type=time

_Workflow 2:_

    nodelist:aggregator1
    component:filter:args:hostname=c01;data_group=coretemp
    component:window:args:win_size=10;compute=sd;type=counter

The first workflow specifies a timing window of 1 hour to do average for the coretemp sensor data of node c02, while the second workflow specifies a counter window of 10 samples to do the standard deviation for the coretemp sensor data of node c01.
 
__4.2 Window plugin attributes:__

* __win_size:__ (_Required_) size of the window
* __compute:__ (_Required_) the type of computation (average, min, max, sd). For now, only one computation is allowed in one workflow step.
* __type:__ (_Optional_) the type of the window. By default it is _time_ window

    type=time: the window is a time window. By default, the unit of the win_size is second. When the users want to do computation with units of minute, hour, day. We are not supporting units longer than day(e.g. weeks, months, years, etc). The users can either set the unit with a unit attribute, or convert the value to second. For example: win_size=3600 means a one-hour window, this equals to set: win_size=1;unit=hour

    type=counter: the window is a counter window, meaning the number of samples. For example: win_size=200 means doing computation for the last 200 samples. 

##5. Count Over Time Threshold (cott)

This analytics plugin is designed to take the incoming absolute count data during sampling and will fire an event when the calculated count delta is greater than or equal to the threshold value within the specified time window.  The 'errcounts' sensor plugin can be used as direct input to this analytic plugin.

Understand that the input values for a given data label (or key) is the current count and not a change of the count since the last data associated with the label.  This means if you get a change value you will need an accumulator-like plugin to get a count to input into this plugin.

The output of this plugin is the incoming data.  It only examines and triggers events.

__5.1 Sample Workflow File:__

    nodelist:node[3:1-254]
    component:filter:args:data_group=errcounts
    component:cott:args:severity=error;fault_type=soft;store_event=yes;notifier_action=email;label_mask=CPU_SrcID#*_Channel#*_DIMM#*_CE;time_window=60s;count_threshold=10

In this example, the filter returns only data from the errcounts plugin to use as input to the cott analytics plugin.  The plugin looks for a count change of 10 within a time window of 60 seconds.  The example when triggered will fire an event of "error" severity and as a "soft" fault.  It also will store the event in the event database.

__5.2 Workflow Arguments:__

* __label_mask__ Required; This specifies the data labels (key) to match using wildcard syntax.
* __count_threshold__  Required; This denotes the number of new item counts within the time_window which after the the event is fired (inclusive).
* __severity__ Optional; This defaults to "error" and can only be "emerg", "alert", "crit", "error", "warn".
* __fault_type__ Optional; This defaults to "hard" and can only be "hard" or "soft".
* __store_event__ Optional; This defaults to "yes" and can only be "yes" or "no".
* __notifier_action__  Optional; This defaults to "none" and can only be "none", "email", "syslog".
* __time_window__ Optional; This defaults to 1 second; the format is a number followed by a character denoting the unit of the number:
 * __s__ - seconds
 * __m__ - minutes
 * __h__ - hours
 * __d__ - days


##6. Spatial
The spatial plugin is used in the purpose of doing spatial (not temporal) data aggregations across a rack or a sub-group of nodes of a rack. The groups of nodes can be defined in the logical group configuration file. When doing the computation for a group of noes, there will be one sample per node for all the nodes. 

__6.1 Sample Workflow File:__

    nodelist:aggregator1
    component:filter:args:hostname=$rack1;data_group=coretemp
    component:spatial:args:nodelist=$rack1;compute=average;interval=10;timeout=20

The above workflow does average for the coretemp sensor data for all the compute nodes defined in group "rack1" in logical group, every 10 seconds. The logical group file maybe as follows:

    group name=rack1
    member list=node1,node2,node3,...,node256

The definition of the attributes are specified in the next section.

__6.2 Spatial Plugin Attributes:__

* __nodelist:__ (_Required_) The list of nodes that the aggregation will be conducted
* __compute:__ (_Required_) the type of computation (average, min, max, sd). For now, only one computation is allowed in one workflow step.
* __interval:__ (_Optional_) It is a timing attribute with the unit of "second". It indicates the "sleeping lenth" between 2 consecutive cycling. This is to give user the ability to control the granularity of cycling if he/she does not want the cycling to go as fast as it can go. The default value is 60 seconds.
* __timeout:__ (_Optional_) It is a timing attribute with the unit of "second". It means the maximum waiting time upon the first sample of a cycle comes before doing the computation. In the case of node failure happens, this attribute avoids waiting endlessly. The default value is 60 seonds. 


##7. DB Storage Attribute

The role of the DB storage attribute is to store the analyzed data in the database. This attribute is optional and will be activated only if user has requested the data to be stored in the database.
Example

    component:average:args:db=yes


##8. Store raw sensor data

Using an MCA parameter from the analytics code, sensor raw data logging to Database is controlled. The MCA parameter that controls the sensor raw data logging to DB is "analytics\_base\_store\_raw\_data"

To turn the sensor raw data logging to true, use the below command

    orcmd --omca analytics_base_store_raw_data true

To turn the sensor raw data logging to false, use the below command

    orcmd --omca analytics_base_store_raw_data false


##9. Store only events

Using an MCA parameter from the analytics code, event data logging to Database is controlled. The MCA parameter that controls the event data logging to DB is "analytics\_base\_store\_event\_data"

To turn the event data logging to true, use the below command

    orcmd --omca analytics_base_store_event_data true

To turn the event data logging to false, use the below command

    orcmd --omca analytics_base_store_event_data false
    
##10. Suppress repeat

The suppress repeat MCA attribute specifies the time period during which all repeat events will be suppressed. An event is considered a repeat if reporter, severity, type and key value match those of an earlier event. The MCA parameter that controls the event data logging to DB is "analytics\_base\_suppress\_repeat"

To suppress repeat events for a time interval T, use below command

    orcmd --omca analytics_base_suppress_event <T>
    
To suppress a certain category or severity of events at a different rate, every plugin supports additional attributes

* __suppress_repeat:__ (_Required_) Enables suppress repeats for specific plugins. If no additional attributes are specified, all events generated by the plugin will be suppressed for time period determined by the analytics_base_suppress_repeat MCA parameter. 
* __category:__ (_Optional_) Event category. Allowed types are HARD_FAULT and SOFT_FAULT
* __severity:__ (_Optional_) Event severity. Allowed types are emerg, alert, crit, error, warn, notice, info and debug. All matching this parameter or with a higher value will be suppressed at a different rate.
* __time:__ (_Optional_) All events that match the severity and category parameters will be suppressed for time period specified using this parameter. All other events will be suppressed time period determined by analytics_base_suppress_repeat MCA parameter.

