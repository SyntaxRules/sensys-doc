This section provides instructions on how to automate data purging when using PostgreSQL.  Specifically, on the table with high growing tuple count (e.g. the `data_sample_raw` table).

The same pattern and technique can be translated to another database dialect, but the code is Postgres specific.  Also the scripts mentioned in this step are available in the ORCM repository `contrib/database` directory.

####2.2.3.1 Background on ORCM Data Purging Requirement
The ORCM process is often configured to continuously collect and store data to the database.  Depending on the configuration, these data can grow rapidly and unbounded.  Hence, there need to be a way to purge out the old data.  

Please note, data archive, if interested, should be done before purging data.  Once the data is purge, it is gone from the database.  The ORCM PostgreSQL schema (orcmdb_psql.sql) provides two ways to pure out data (based on timestamp attribute):

* A stored procedure to delete data after certain time interval from current time
* A trigger based partition with dropping partitions that has exceeded certain time interval from the timestamp of the latest tuple

Each of these will be shown in detail on how to deploy in the following sections.

####2.2.3.2 Purging with 'purge_data()' Function
The `purge_data()` function provides the simple way to deleting data after the specified time interval from current time.  At the high level, this function resolves an SQL DELETE statement:

```
DELETE FROM table_name WHERE timestamp_column < CURRENT_TIMESTAMP - interval '<interval_number> <interval_unit>';
```

Below is an example of the SQL statement invocation of the `purge_data()` function.  This statement would DELETE all rows in the `data_sample_raw` table WHERE `time_stamp` column having value of less than 6 months from the current time.

```
SELECT purge_data('data_sample_raw', 'time_stamp', 6, 'MONTH');
```

The SQL statement example above can be schedule to run on a regular basis using pgAgent.  Alternative, the statement can also be executed as a shell script using `psql` tool.  It then can easily be scheduled to run on a regular basis using any schedule tool such as `crontab`.  
 
```
psql -U username -d database -c "SELECT purge_data('data_sample_raw', 'time_stamp', 6, 'MONTH');"
```

####2.2.3.3 Purging with Dropping Partition
Another way to purge data is to DROP the table rather than DELETE tuple from a table as in how the `purge_data()` function works.  We do not want to DROP the whole table, but only a section of the table.  There is no such thing as DROP a section of a table; However, DROP of a table partition is possible.  So for this to work, we will first enable partition on the table and DROP any out dated partition.
 
Below is the detail instruction on how to enable partition on a given table with and without the helper script.

######2.2.3.3.1 Partition without Helper (psql)
Below is an example of the SQL statement invocation on the generated partition code for the `data_sample_raw` table on the `time_stamp` column by DAY and keep the last 180 partitions.

1. Generate the partition DDL (data definition language) code.

    ```
    SELECT generate_partition_triggers_ddl('data_sample_raw', 'time_stamp', 'DAY', 180)';
    ```
2. Review the generated code  

3. Turn on auto dropping old partition by uncomment the `-- EXECUTE('DROP TABLE...` statement within the generated code.  By default, the generated DROP TABLE statement is commented out.
 
4. Execute the modified and reviewed version of the generated code

######2.2.3.3.2 Partition with Helper Script
The same process can be done using `enable_pg_partition.py`, a python helper script that would perform the step above.  This pythons script uses SQLAlchemy to connect to the PostgreSQL database and execute the SQL DML (data manipulation language) and DDL.  For this to work, the system needs to have the following python modules installed:
* SQLAlchemy - Database Abstraction Library
* psycopg2 - Python-PostgreSQL

Because this use SQLAlchemy to connect to the database, the database connection string will need to be in the format of [SQLAlchemy database URL syntax](http://docs.sqlalchemy.org/en/latest/core/engines.html#database-urls) 

The helper script expects the PostgreSQL database URL to be stored in the environment variable `PG_DB_URL`, see example below on how to set it.

```
export PG_DB_URL=dialect+driver://username:password@host:port/database
```

Again, before executing this helper script, be sure the review the main() function and update the data purge parameters.  Below is an example of enabling trigger based partition for the `data_sample_raw` table on the `time_stamp` column by DAY and keeping the last 10 partitions (based on the timestamp of the new tuple)

```
def main():
    """Main driver"""
    table_name = "data_sample_raw"
    column_name = "time_stamp"
    interval = "DAY"
    interval_to_keep = 10
    enable_purging = True

    disable_partition_trigger(table_name)

    enable_partition_trigger(table_name, column_name, interval,
                             interval_to_keep, enable_purging=enable_purging)
```

######2.2.3.3.3 Managing Partitions
Table partitioning in PostgreSQL database is based on table inheritance.  Each new partition of a main table is a whole new table that inherits the schema of the main table.  In the examples above, `data_sample_raw` is the main table that does not hold any record.  All records are stored in its corresponding partition.  When querying data from the main table, `data_sample_raw`, PostgreSQL automatic query the data from the partitioned tables.

Partition can be un-linked from the main table (no longer a part of the main table), this effectively provides an archive mechanism, or it can be DROPPED from the schema.

####2.2.3.4 Recommendation
The simple `purge_data()` function ultimately is an SQL DELETE statement.  The process of deleting records usually involve some logging as the overhead.  On the other side of dropping the whole partition of a table is close to instantaneous (i.e. quick format).  However, this requires partitioning a table.  Partition of a table in turn incurs a cost of executing a trigger to route each new tuple to the respective partition.
  
The recommendation is to use the `purge_data()` approach as this may already be sufficient for the data purging.  The data partitioning approach is easier on archive and potentially improve performance on accessing the data since the data is 'partitioned' to different tables.
